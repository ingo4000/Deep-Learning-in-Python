{"cells":[{"cell_type":"markdown","metadata":{},"source":["\n","\n","# Welcome to Natural Language Processing!\n","\n","This course is about Natural Language Processing (NLP) with Keras/TensorFlow and the HuggingFace collection of language models. NLP is the field in statistical learning that teaches computers how to 'understand' language. (Or at least, how to make it appear it does &#128521;.)\n","\n","In this course you will learn to work with the most current NLP techniques that build on (deep) neural networks. You will\n","\n","- Use state of the art Transformer models to train a document classifier with Keras\n","- Train your own word vector embeddings \n","- Learn the fundamentals behind autoregressive language models using RNNs \n","- Use pretrained models to complete sentences \n","- Let state of the art models from Huggingface answer questions \n","\n","\n","If you've taken the Introduction to Deep Learning course, you'll know everything you need to be successful.\n","\n","Now let's get started!\n","\n","# Introduction"]},{"cell_type":"markdown","metadata":{},"source":["The `transformers` library gives access to a host of pretrained language models for various NLP tasks:\n","\n","- `feature-extraction` (get the vector representation of a text)\n","- `fill-mask`\n","- `ner` (named entity recognition)\n","- `question-answering`\n","- `sentiment-analysis`\n","- `summarization`\n","- `text-generation`\n","- `translation`\n","- `zero-shot-classification`\n","\n","The easiest way to access these models is by means of the `pipeline` object. The `pipeline` object allows you to _instantiate_ a model as an object that you can call on a string of text. The object takes care of preprocessing the text (tokenization, etc.), running the model, and turning the output into and easy understandable and use format.\n","\n","Let's take a look at some of these models.\n","\n","# Sentiment analysis\n","\n","The first item on the list that we'll try is `sentiment-analysis`"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-11-23T08:41:12.473579Z","iopub.status.busy":"2023-11-23T08:41:12.472821Z","iopub.status.idle":"2023-11-23T08:41:41.357858Z","shell.execute_reply":"2023-11-23T08:41:41.356179Z","shell.execute_reply.started":"2023-11-23T08:41:12.473480Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\lutzh\\OneDrive - UvA\\UVA\\Python\\Deep Learning in Python\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]},{"name":"stdout","output_type":"stream","text":["WARNING:tensorflow:From c:\\Users\\lutzh\\OneDrive - UvA\\UVA\\Python\\Deep Learning in Python\\.venv\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n","\n"]},{"name":"stderr","output_type":"stream","text":["No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n","Using a pipeline without specifying a model name and revision in production is not recommended.\n","config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 629/629 [00:00<00:00, 622kB/s]\n","c:\\Users\\lutzh\\OneDrive - UvA\\UVA\\Python\\Deep Learning in Python\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\lutzh\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n","To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n","  warnings.warn(message)\n","model.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 268M/268M [00:16<00:00, 16.6MB/s] \n"]},{"name":"stdout","output_type":"stream","text":["WARNING:tensorflow:From c:\\Users\\lutzh\\OneDrive - UvA\\UVA\\Python\\Deep Learning in Python\\.venv\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n","\n"]},{"name":"stderr","output_type":"stream","text":["All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n","\n","All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n","tokenizer_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48.0/48.0 [00:00<?, ?B/s]\n","vocab.txt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232k/232k [00:00<00:00, 1.41MB/s]\n"]}],"source":["from transformers import pipeline\n","\n","classifier = pipeline(\"sentiment-analysis\")"]},{"cell_type":"markdown","metadata":{},"source":["As you can see, the first thing that the call to `pipeline` does is (besides complaining that we weren't specific about the model we wantedâ€”more on that later) to download files. These are the pretrained model files that are stored on the [ðŸ¤— HuggingFace repository](https://huggingface.co). If they are not available (yet) in your current working environment, they need to be downloaded. The `transformer` library takes care of this for you _automagically_.\n","\n","Let's see what `classifier` does:"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-11-23T08:41:52.856733Z","iopub.status.busy":"2023-11-23T08:41:52.855978Z","iopub.status.idle":"2023-11-23T08:41:53.185670Z","shell.execute_reply":"2023-11-23T08:41:53.184014Z","shell.execute_reply.started":"2023-11-23T08:41:52.856691Z"},"trusted":true},"outputs":[{"data":{"text/plain":["[{'label': 'POSITIVE', 'score': 0.9998846054077148},\n"," {'label': 'POSITIVE', 'score': 0.9998706579208374},\n"," {'label': 'NEGATIVE', 'score': 0.9995550513267517},\n"," {'label': 'NEGATIVE', 'score': 0.9915013313293457}]"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["classifier(\n","    [\"It is a new day, the sun is shining, I feel good.\", \n","     \"I love this NLP stuff!\",\n","     \"This muscle ache is killing me\",\n","     \"I can reassure you that your headaches are now a sorrow of the past.\"])"]},{"cell_type":"markdown","metadata":{},"source":["It's clear that this model is not very good at recognizing double negatives... (But then again, are humans good at that?)"]},{"cell_type":"markdown","metadata":{},"source":["# Question answering\n","\n","Next we'll try `question-answering`. In question answering, the model takes two strings:\n","\n","1. A contextâ€”a text in which the answer can be found, and\n","2. A question that is _assumed to be_ answerable with the text in the context string.\n","\n","The model was trained to predict the start and end of the segment (in terms of the location of the first and last character in the _context_ that holds the answer to the _question_.\n","\n","So for instance, if\n","\n","`context` = _\"Lucy had named the kitten Purr.\"_\n","\n","and the question is _\"What was the name of the cat?\"_, the model will try to predict `(26,30)`, because those are the start and end of the substring that contains \"Purr\"."]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-11-23T08:42:02.840045Z","iopub.status.busy":"2023-11-23T08:42:02.839467Z","iopub.status.idle":"2023-11-23T08:42:25.438452Z","shell.execute_reply":"2023-11-23T08:42:25.437151Z","shell.execute_reply.started":"2023-11-23T08:42:02.839994Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["No model was supplied, defaulted to distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert-base-cased-distilled-squad).\n","Using a pipeline without specifying a model name and revision in production is not recommended.\n","config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 473/473 [00:00<?, ?B/s] \n","model.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 261M/261M [00:16<00:00, 15.5MB/s] \n","All PyTorch model weights were used when initializing TFDistilBertForQuestionAnswering.\n","\n","All the weights of TFDistilBertForQuestionAnswering were initialized from the PyTorch model.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForQuestionAnswering for predictions without further training.\n","tokenizer_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29.0/29.0 [00:00<?, ?B/s]\n","vocab.txt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 213k/213k [00:00<00:00, 1.19MB/s]\n","tokenizer.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 436k/436k [00:00<00:00, 4.71MB/s]\n"]}],"source":["question_answerer = pipeline(\"question-answering\")"]},{"cell_type":"markdown","metadata":{},"source":["Notice that a different model is downloaded. For most tasks, a dedicated model needs to be downloaded. Some of the models are really large (more than 1GB) and so we won't try all of them here. (But feel free to play around and have some fun with them! Basic usage documentation can be found on [ðŸ¤— HuggingFace](https://huggingface.co).)"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-11-23T08:42:40.214714Z","iopub.status.busy":"2023-11-23T08:42:40.213589Z","iopub.status.idle":"2023-11-23T08:42:40.304441Z","shell.execute_reply":"2023-11-23T08:42:40.303316Z","shell.execute_reply.started":"2023-11-23T08:42:40.214664Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["{'score': 0.5847041010856628, 'start': 32, 'end': 43, 'answer': 'kitten Purr'}\n","{'score': 0.466654509305954, 'start': 13, 'end': 17, 'answer': 'Lucy'}\n"]}],"source":["print(question_answerer(\n","    context=\"His daughter Lucy had named the kitten Purr\",\n","    question=\"What was the name of the cat?\",\n","))\n","print(question_answerer(\n","    context=\"His daughter Lucy had named the kitten Purr\",\n","    question=\"Who owns the cat?\",\n","))"]},{"cell_type":"markdown","metadata":{},"source":["The model does seem to 'know' to distinguish between the two names in the sentence. Let's try something more complicated:"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-11-23T08:43:23.938880Z","iopub.status.busy":"2023-11-23T08:43:23.938381Z","iopub.status.idle":"2023-11-23T08:43:24.077944Z","shell.execute_reply":"2023-11-23T08:43:24.076207Z","shell.execute_reply.started":"2023-11-23T08:43:23.938841Z"},"trusted":true},"outputs":[{"data":{"text/plain":["[{'score': 0.49840980768203735,\n","  'start': 111,\n","  'end': 148,\n","  'answer': 'cognitive and mathematical psychology'},\n"," {'score': 0.8162108659744263,\n","  'start': 175,\n","  'end': 186,\n","  'answer': 'accelerated'}]"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["TEXT = \"\"\"Key developments in artificial neural networks and deep learning came not from \n","    computer science, but from cognitive and mathematical psychology. Computer scientists \n","    accelerated the development with more powerful computers and implementation frameworks.\"\"\"\n","\n","[  question_answerer(question=\"In which academic field was deep learning developed?\", context = TEXT), \n","   question_answerer(question=\"What was the role of computer science?\",context=TEXT)]\n"]},{"cell_type":"markdown","metadata":{},"source":["# Text generation with GPT2\n","\n","Remember the complaint about not being specific about the model? We can fix that by specifying a specific model (in stead of relying on default choices). Here we'll use the ðŸ¤— HuggingFace simplified version of the famous GPT2 model (the precursor to GPT3) to generate some text."]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-11-23T08:43:42.827962Z","iopub.status.busy":"2023-11-23T08:43:42.827495Z","iopub.status.idle":"2023-11-23T08:44:12.941884Z","shell.execute_reply":"2023-11-23T08:44:12.940230Z","shell.execute_reply.started":"2023-11-23T08:43:42.827925Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 762/762 [00:00<00:00, 477kB/s]\n","model.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 353M/353M [00:20<00:00, 17.4MB/s] \n","All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n","\n","All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n","vocab.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.04M/1.04M [00:00<00:00, 11.1MB/s]\n","merges.txt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 456k/456k [00:00<00:00, 2.78MB/s]\n","tokenizer.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.36M/1.36M [00:00<00:00, 3.19MB/s]\n"]}],"source":["from transformers import pipeline\n","\n","generator = pipeline(\"text-generation\", model=\"distilgpt2\")"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-11-23T08:46:00.808504Z","iopub.status.busy":"2023-11-23T08:46:00.807531Z","iopub.status.idle":"2023-11-23T08:46:05.454239Z","shell.execute_reply":"2023-11-23T08:46:05.452834Z","shell.execute_reply.started":"2023-11-23T08:46:00.808451Z"},"trusted":true},"outputs":[{"data":{"text/plain":["[{'generated_text': 'Welcome to the natural language processing (NLP) module of Deep Learning in Python. In this course, you will learn vernacular, German and German, Spanish, and English as well as Latin-related syntax concepts.\\n\\n\\n\\n\\nWeâ€ºll try to put some great resources down the way we want to start learning Python.\\nWe have now come up with a program that lets us create our own learning program. Now, we need something simple.\\nCreating our own learning'},\n"," {'generated_text': 'Welcome to the natural language processing (NLP) module of Deep Learning in Python. In this course, you will learn \\xa0 this new Python programming language by first developing\\xa0 this new language by becoming\\xa0 the first to use Python in your machine learning.\\nThe lessons for using Python in your machine learning class will help you develop this new language in your Machine Learning language by exploring what the language is and what your machine learning learning system is capable of.\\nThere are already\\xa0 many different types of'}]"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["generator(\"Welcome to the natural language processing (NLP) module of Deep Learning in Python. In this course, you will learn \",\n","    max_length=100, num_return_sequences=2, pad_token_id=50256)"]},{"cell_type":"markdown","metadata":{},"source":["Not exactly sensible, nor Shakespeare, and not exactly GPT3 either, but you get the drift.\n","\n","\n","# State of the art NLP models\n","\n","All of these models (plus the models for the NLP tasks listed earlier) were built using what is known as a _Transformer_ architecture. We'll come back to what that entails in more detail in a later tutorial. For now, let's recognize that these types of language models are currently (in 2022) the absolute state of the art. \n","\n","The models we've seen here are not the best available models, but they do share the same basic architecture with the models that are the bestâ€”what's sets the latter apart is not only their performance on a litany of tasks, but certainly also their size: The models here were limited to under 500Mb in file size to store the weights, but the best models are several Gb and larger. \n","\n","All of these models have been trained on enormous text corpora, such as the [Common Crawl](https://commoncrawl.org/), [Wikipedia (EN)](https://huggingface.co/datasets/wikipedia), or [WebText (EN)](https://paperswithcode.com/dataset/webtext), to repressent the statistical dependancies between words, in an unsupervised way. \n","\n","## How?\n","\n","You may wonder how, and the answer is relatively simple: by predicting words from context words. This can take various forms, but let's focus on two:\n","\n","1. Fill in the blank: Predict the (likelihood of) word that goes in the empty space in for example `\"the cat ___ the mouse\"` (e.g., `ate`, `killed`, `caught` are all likely, but `painted`, `sung`, `addopted` are all unlikely).\n","2. Predict the next token, given the sequence of tokens so far, for each token in a piece of text. (We say token here instead of word, because a token may also be a punctuation character, or the _End Of Sequence_ token `<EOS>`. For example, in `\"the cat chased the mouse\"` the model tries to predict \n","  \n","  $P({\\tt the})$,\n","\n","  $P({\\tt cat}|{\\tt the})$,\n","\n","  $P({\\tt chased}|{\\tt the, cat})$,\n","\n","  $P({\\tt the}|{\\tt the, cat, chased})$,\n","\n","  $P({\\tt mouse}|{\\tt the, cat, chased, the})$, and\n","\n","  $P({\\tt <\\!EOS\\!>}|{\\tt the, cat, chased, the, mouse})$\n","\n","The difference between the two methods isn't very large, but notice that the latter method can learn the probability for the entire sentence: From the rules of conditional probability \n","\n","$$P(A,B) = P(A)P(B|A),$$ and more generally, $$P(A,B,C,\\ldots) = P(A)P(B,C,\\ldots|A).$$\n","\n","Applying this recursively, we can compute \n","\n","$$P(A,B,C,\\ldots) = P(A)\\cdot P(B|A)\\cdot   P(C|A,B)\\cdot P(\\ldots|A,B,C) \\cdot \\cdots$$\n","\n","Hence, if we took a random sentence from the text of say the internet, we can write the probability that the sentence is equal to `\"the cat chased the mouse\"` as\n","\n","$$P({\\tt the, cat, chased, the, mouse, <\\!EOS\\!>}) =  \n","P({\\tt the})\\, \n","P({\\tt cat}|{\\tt the})\\,\n","P({\\tt chased}|{\\tt the, cat})\\, P({\\tt the}|{\\tt the, cat, chased})\\,\n","P({\\tt mouse}|{\\tt the, cat, chased, the})\\, \n","P({\\tt <\\!EOS\\!>}|{\\tt the, cat, chased, the, mouse}).$$\n","\n","Notice that we can do this with any sentence. Specifically, any sentence of any length in principle (in practice computational problems mount as sentences become larger). \n","\n","Contrast this to the first method of training (the fill-in-the-blank method): This method only learns the conditional probabilities of the form \n","\n","$$P(X_0 | X_{p}, \\ldots, X_{-1}, X_1, \\ldots, X_q).$$\n","\n","Here $p$ and $q$ are integers that specify a window around the focus token $X_0$. Usually, $p \\lt 0 \\le q$, but this doesn't have to be the case. For our running example this would be, for instance,\n","\n","$$P(X_0 = {\\tt chased} | X_{-2}={\\tt the}, X_{-1}={\\tt cat}, X_1={\\tt the}, X_2={\\tt mouse}),$$ \n","\n","$$P(X_0 = {\\tt caught} | X_{-2}={\\tt the}, X_{-1}={\\tt cat}, X_1={\\tt the}, X_2={\\tt mouse}),$$ \n","\n","$$P(X_0 = {\\tt killed} | X_{-2}={\\tt the}, X_{-1}={\\tt cat}, X_1={\\tt the}, X_2={\\tt mouse}),$$ \n","\n","etc.\n"]},{"cell_type":"markdown","metadata":{},"source":["# Your Turn\n","\n","Now that you've seen a few uses of the ðŸ¤— HuggingFace `transformer` library for NLP, it's your turn to try it to teach a neural network to [recognize sentences with the same meaning](https://www.kaggle.com/code/datasniffer/nlp-ex1/).\n","\n","\n","---\n","\n","<small>\n","    \n","_For Deep Learning in Python (2022)._\n","    \n","</small>\n"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":134715,"sourceId":320111,"sourceType":"datasetVersion"}],"dockerImageVersionId":30260,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.6"}},"nbformat":4,"nbformat_minor":4}
